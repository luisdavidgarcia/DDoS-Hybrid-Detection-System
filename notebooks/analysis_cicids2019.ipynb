{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIC-IDS-2019 Dataset Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For getting paths\n",
    "import os\n",
    "\n",
    "# For data manipulation and preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# For feature engineering\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# For model building\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# For model evaluation\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For oversampling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For statistical analysis\n",
    "import scipy\n",
    "\n",
    "# For timing\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aquire the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Users/lucky/GitHub/DDoS-Hybrid-Detection-System/datasets/cic_ids_2019'\n",
    "\n",
    "day1_dir = '/Users/lucky/GitHub/DDoS-Hybrid-Detection-System/datasets/cic_ids_2019/DoS/DDoS/03-11'\n",
    "day2_dir = '/Users/lucky/GitHub/DDoS-Hybrid-Detection-System/datasets/cic_ids_2019/DoS/DDoS/01-12'\n",
    "\n",
    "data_frames = []\n",
    "\n",
    "# Flag to indicate if the header should be included\n",
    "include_header = True\n",
    "\n",
    "# Walk through the directory and load each CSV file into a dataframe\n",
    "for root, dirs, files in os.walk(data_dir):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            print(f\"Reading {file}\")\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                if include_header:\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    include_header = False \n",
    "                else:\n",
    "                    df = pd.read_csv(file_path, header=None, skiprows=1)\n",
    "                    df.columns = data_frames[0].columns\n",
    "                data_frames.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "# Combine all dataframes into a single dataframe\n",
    "combined_df = pd.concat(data_frames, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe the data\n",
    "print(combined_df.shape)\n",
    "print(combined_df.head())\n",
    "print(combined_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Columns that Do Not Exist in Packet Capture Data/Ungeneralizable Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns that have potential to bias the model\n",
    "combined_df.drop(\n",
    "    [\n",
    "        'Flow ID', \n",
    "        ' Source IP', \n",
    "        ' Destination IP', \n",
    "        ' Timestamp', \n",
    "        'SimillarHTTP', \n",
    "        'Unnamed: 0', \n",
    "        ' Inbound'\n",
    "    ], \n",
    "    axis=1,\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removal Duplicates, NAN, +/-INF Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maintain a copy of the original dataframe\n",
    "original_df = combined_df.copy()\n",
    "\n",
    "# Remove duplicates\n",
    "combined_df.drop_duplicates(inplace=True)\n",
    "# Replace infinity with NaN\n",
    "combined_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "# Remove columns with all NaN values\n",
    "combined_df.dropna(axis=1, how='all', inplace=True)\n",
    "# Remove rows with any NaN values\n",
    "combined_df.dropna(inplace=True)\n",
    "# Acquire column names that contain object data types\n",
    "object_cols = combined_df.select_dtypes(include=['object']).columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "watchtower_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
